{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca776f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "import os\n",
    "import tvm\n",
    "from tvm import te\n",
    "import vta\n",
    "import numpy as np\n",
    "from tvm import rpc\n",
    "from tvm.contrib import utils\n",
    "from vta.testing import simulator\n",
    "from collections import namedtuple\n",
    "from tvm import relay\n",
    "from tvm import autotvm\n",
    "from tvm import topi\n",
    "import tvm.topi.testing\n",
    "# Load VTA parameters from the 3rdparty/vta-hw/config/vta_config.json file\n",
    "env = vta.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eeacb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"10.100.82.139\"\n",
    "port = int(os.environ.get(\"VTA_RPC_PORT\", \"9091\"))\n",
    "\n",
    "# We configure both the bitstream and the runtime system on the Pynq\n",
    "# to match the VTA configuration specified by the vta_config.json file.\n",
    "if env.TARGET == \"pynq\" or env.TARGET == \"de10nano\" or env.TARGET == \"zcu104\":\n",
    "\n",
    "    # Make sure that TVM was compiled with RPC=1\n",
    "    assert tvm.runtime.enabled(\"rpc\")\n",
    "    remote = rpc.connect(host, port)\n",
    "\n",
    "    # Reconfigure the JIT runtime\n",
    "    vta.reconfig_runtime(remote)\n",
    "\n",
    "    # Program the FPGA with a pre-compiled VTA bitstream.\n",
    "    # You can program the FPGA with your own custom bitstream\n",
    "    # by passing the path to the bitstream file instead of None.\n",
    "    #vta.program_fpga(remote, bitstream=\"/home/srchand/Desktop/research/TVM_Intel_Fork/tvm/vta/sri_scripts/bitstreams/vta_zcu104_wrapper.bit\")\n",
    "\n",
    "# In simulation mode, host the RPC server locally.\n",
    "elif env.TARGET in [\"sim\", \"tsim\"]:\n",
    "    remote = rpc.LocalSession()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84838075",
   "metadata": {},
   "outputs": [],
   "source": [
    "Workload = namedtuple(\n",
    "    \"Conv2DWorkload\",\n",
    "    [\n",
    "        \"batch\",\n",
    "        \"height\",\n",
    "        \"width\",\n",
    "        \"in_filter\",\n",
    "        \"out_filter\",\n",
    "        \"hkernel\",\n",
    "        \"wkernel\",\n",
    "        \"hpad\",\n",
    "        \"wpad\",\n",
    "        \"hstride\",\n",
    "        \"wstride\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# ResNet18 workloads\n",
    "resnet_wkls = [\n",
    "    # Workloads of resnet18 on imagenet\n",
    "    # ('resnet-18.C1',  Workload(env.BATCH, 224, 224, 3,   64,  7, 7, 3, 3, 2, 2)),\n",
    "    (\"resnet-18.C2\", Workload(env.BATCH, 56, 56, 64, 64, 3, 3, 1, 1, 1, 1)),\n",
    "    # (\"resnet-18.C3\", Workload(env.BATCH, 56, 56, 64, 128, 3, 3, 1, 1, 2, 2)),\n",
    "    # (\"resnet-18.C4\", Workload(env.BATCH, 56, 56, 64, 128, 1, 1, 0, 0, 2, 2)),\n",
    "    # (\"resnet-18.C5\", Workload(env.BATCH, 28, 28, 128, 128, 3, 3, 1, 1, 1, 1)),\n",
    "    # (\"resnet-18.C6\", Workload(env.BATCH, 28, 28, 128, 256, 3, 3, 1, 1, 2, 2)),\n",
    "    # (\"resnet-18.C7\", Workload(env.BATCH, 28, 28, 128, 256, 1, 1, 0, 0, 2, 2)),\n",
    "    # (\"resnet-18.C8\", Workload(env.BATCH, 14, 14, 256, 256, 3, 3, 1, 1, 1, 1)),\n",
    "    # (\"resnet-18.C9\", Workload(env.BATCH, 14, 14, 256, 512, 3, 3, 1, 1, 2, 2)),\n",
    "    # (\"resnet-18.C10\", Workload(env.BATCH, 14, 14, 256, 512, 1, 1, 0, 0, 2, 2)),\n",
    "    # (\"resnet-18.C11\", Workload(env.BATCH, 7, 7, 512, 512, 3, 3, 1, 1, 1, 1)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de6289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: we need a custom clip operator to circumvent a pattern detection limitation\n",
    "@tvm.te.tag_scope(tag=topi.tag.ELEMWISE)\n",
    "def my_clip(x, a_min, a_max):\n",
    "    \"\"\"Unlike topi's current clip, put min and max into two stages.\"\"\"\n",
    "    const_min = tvm.tir.const(a_min, x.dtype)\n",
    "    const_max = tvm.tir.const(a_max, x.dtype)\n",
    "    x = te.compute(x.shape, lambda *i: tvm.te.min(x(*i), const_max), name=\"clipA\")\n",
    "    x = te.compute(x.shape, lambda *i: tvm.te.max(x(*i), const_min), name=\"clipB\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def run_conv2d(env, remote, wl, target, check_correctness=True, print_ir=False, samples=4):\n",
    "\n",
    "    # Workload assertions\n",
    "    assert wl.hpad == wl.wpad\n",
    "\n",
    "    # Perform packing only if we are targeting the accelerator\n",
    "    if \"arm_cpu\" in target.keys:\n",
    "        data_pack = False\n",
    "        layout = \"NCHW\"\n",
    "        conv2d_fcompute = topi.arm_cpu.conv2d_nchw_spatial_pack\n",
    "        conv2d_fschedule = topi.arm_cpu.schedule_conv2d_nchw_spatial_pack\n",
    "    elif \"vta\" in target.keys:\n",
    "        data_pack = True\n",
    "        layout = \"NCHW%dn%dc\" % (env.BATCH, env.BLOCK_IN)\n",
    "        conv2d_fcompute = vta.top.conv2d_packed\n",
    "        conv2d_fschedule = vta.top.schedule_conv2d_packed\n",
    "\n",
    "    # Derive shapes depending upon packing\n",
    "    a_shape = (wl.batch, wl.in_filter, wl.height, wl.width)\n",
    "    w_shape = (wl.out_filter, wl.in_filter, wl.hkernel, wl.wkernel)\n",
    "    b_shape = (wl.batch, wl.out_filter, 1, 1)\n",
    "    if data_pack:\n",
    "        data_shape = (\n",
    "            wl.batch // env.BATCH,\n",
    "            wl.in_filter // env.BLOCK_IN,\n",
    "            wl.height,\n",
    "            wl.width,\n",
    "            env.BATCH,\n",
    "            env.BLOCK_IN,\n",
    "        )\n",
    "        kernel_shape = (\n",
    "            wl.out_filter // env.BLOCK_OUT,\n",
    "            wl.in_filter // env.BLOCK_IN,\n",
    "            wl.hkernel,\n",
    "            wl.wkernel,\n",
    "            env.BLOCK_OUT,\n",
    "            env.BLOCK_IN,\n",
    "        )\n",
    "        bias_shape = (\n",
    "            wl.batch // env.BATCH,\n",
    "            wl.out_filter // env.BLOCK_OUT,\n",
    "            1,\n",
    "            1,\n",
    "            env.BATCH,\n",
    "            env.BLOCK_OUT,\n",
    "        )\n",
    "    else:\n",
    "        data_shape = a_shape\n",
    "        kernel_shape = w_shape\n",
    "        bias_shape = b_shape\n",
    "    data = te.placeholder(data_shape, name=\"data\", dtype=env.inp_dtype)\n",
    "    kernel = te.placeholder(kernel_shape, name=\"kernel\", dtype=env.wgt_dtype)\n",
    "    bias = te.placeholder(bias_shape, name=\"bias\", dtype=env.acc_dtype)\n",
    "    padding = relay.nn.get_pad_tuple2d((wl.hpad, wl.wpad))\n",
    "\n",
    "    # Define base computation schedule\n",
    "    with target:\n",
    "        if data_pack:\n",
    "            res = conv2d_fcompute(\n",
    "                data, kernel, (wl.hstride, wl.wstride), padding, (1, 1), layout, env.acc_dtype\n",
    "            )\n",
    "        else:\n",
    "            res = conv2d_fcompute(\n",
    "                data, kernel, (wl.hstride, wl.wstride), padding, (1, 1), env.acc_dtype\n",
    "            )\n",
    "        res = topi.right_shift(res, 8)\n",
    "        res = topi.add(res, bias)\n",
    "        res = my_clip(res, 0, (1 << env.OUT_WIDTH - 1) - 1)\n",
    "        res = topi.cast(res, env.out_dtype)\n",
    "        # Derive base schedule\n",
    "        s = conv2d_fschedule([res])\n",
    "        if print_ir:\n",
    "            print(vta.lower(s, [data, kernel, bias, res], simple_mode=True))\n",
    "\n",
    "    # Derive number of ops\n",
    "    fout_height = (wl.height + 2 * wl.hpad - wl.hkernel) // wl.hstride + 1\n",
    "    fout_width = (wl.width + 2 * wl.wpad - wl.wkernel) // wl.wstride + 1\n",
    "    num_ops = 2 * wl.batch * fout_height * fout_width * wl.hkernel * wl.wkernel * wl.out_filter * wl.in_filter\n",
    "\n",
    "    # @memoize(\"vta.tests.test_benchmark_topi.conv2d.verify_nhwc\")\n",
    "    def get_ref_data():\n",
    "        # derive min max for act, wgt, and bias types (max non inclusive)\n",
    "        a_min, a_max = 0 - (1 << (env.INP_WIDTH - 1)), (1 << (env.INP_WIDTH - 1))\n",
    "        w_min, w_max = 0 - (1 << (env.WGT_WIDTH - 1)), (1 << (env.WGT_WIDTH - 1))\n",
    "        b_min, b_max = 0 - 1 << (env.INP_WIDTH + env.WGT_WIDTH - 2), 1 << (env.INP_WIDTH + env.WGT_WIDTH - 2)\n",
    "        a_np = np.random.randint(a_min, a_max, size=a_shape).astype(data.dtype)\n",
    "        w_np = np.random.randint(w_min, w_max, size=w_shape).astype(kernel.dtype)\n",
    "        b_np = np.random.randint(b_min, b_max, size=b_shape).astype(env.acc_dtype)\n",
    "        r_np = tvm.topi.testing.conv2d_nchw_python(\n",
    "            a_np.astype(env.acc_dtype),\n",
    "            w_np.astype(env.acc_dtype),\n",
    "            (wl.hstride, wl.wstride),\n",
    "            wl.hpad,\n",
    "        ).astype(env.acc_dtype)\n",
    "        return a_np, w_np, b_np, r_np\n",
    "\n",
    "    # Data in original format\n",
    "    data_np, kernel_np, bias_np, res_ref = get_ref_data()\n",
    "    if data_pack:\n",
    "        data_np = data_np.reshape(\n",
    "            wl.batch//env.BATCH, env.BATCH,\n",
    "            wl.in_filter//env.BLOCK_IN, env.BLOCK_IN,\n",
    "            wl.height, wl.width).transpose((0, 2, 4, 5, 1, 3))\n",
    "        kernel_np = kernel_np.reshape(\n",
    "            wl.out_filter//env.BLOCK_OUT, env.BLOCK_OUT,\n",
    "            wl.in_filter//env.BLOCK_IN, env.BLOCK_IN,\n",
    "            wl.hkernel, wl.wkernel).transpose((0, 2, 4, 5, 1, 3))\n",
    "        bias_np = bias_np.reshape(\n",
    "            wl.batch//env.BATCH, wl.out_filter//env.BLOCK_OUT,\n",
    "            1, 1, env.BATCH, env.BLOCK_OUT)\n",
    "\n",
    "    # Build\n",
    "    if \"vta\" in target.keys:\n",
    "        mod = vta.build(\n",
    "            s, [data, kernel, bias, res], target=target, target_host=env.target_host, name=\"conv2d\"\n",
    "        )\n",
    "    else:\n",
    "        mod = tvm.build(\n",
    "            s, [data, kernel, bias, res], target=target, target_host=env.target_host, name=\"conv2d\"\n",
    "        )\n",
    "    temp = utils.tempdir()\n",
    "    mod.save(temp.relpath(\"conv2d.o\"))\n",
    "    remote.upload(temp.relpath(\"conv2d.o\"))\n",
    "    f = remote.load_module(\"conv2d.o\")\n",
    "    ctx = remote.context(str(target))\n",
    "\n",
    "    res_np = np.zeros(topi.utils.get_const_tuple(res.shape)).astype(res.dtype)\n",
    "    data_arr = tvm.nd.array(data_np, ctx)\n",
    "    kernel_arr = tvm.nd.array(kernel_np, ctx)\n",
    "    bias_arr = tvm.nd.array(bias_np, ctx)\n",
    "    res_arr = tvm.nd.array(res_np, ctx)\n",
    "    time_f = f.time_evaluator(\"conv2d\", ctx, number=samples)\n",
    "\n",
    "    # In vta sim mode, collect simulator runtime statistics\n",
    "    stats = {}\n",
    "    cost = None\n",
    "    \n",
    "    # COMPUTE CONVOLUTION\n",
    "    cost = time_f(data_arr, kernel_arr, bias_arr, res_arr)\n",
    "\n",
    "    # Check correctness\n",
    "    correct = False\n",
    "    if check_correctness:\n",
    "        res_orig = res_arr.asnumpy()\n",
    "        if data_pack:\n",
    "            res_orig = res_orig.transpose((0, 4, 1, 5, 2, 3)).reshape(\n",
    "                wl.batch, wl.out_filter, fout_height, fout_width\n",
    "            )\n",
    "            bias_np = bias_np.transpose((0, 4, 1, 5, 2, 3)).reshape(wl.batch, wl.out_filter, 1, 1)\n",
    "        res_ref = res_ref >> env.WGT_WIDTH\n",
    "        res_ref += bias_np\n",
    "        res_ref = np.clip(res_ref, 0, (1 << env.OUT_WIDTH - 1) - 1)\n",
    "        res_ref = res_ref.astype(env.out_dtype)\n",
    "        correct = np.allclose(res_orig, res_ref)\n",
    "\n",
    "    gops = (num_ops / cost.mean) / float(10 ** 9)\n",
    "    status = \"PASSED\" if correct else \"FAILED\"\n",
    "    if \"arm_cpu\" in target.keys:\n",
    "        device = \"CPU\"\n",
    "    elif \"vta\" in target.keys:\n",
    "        device = \"VTA\"\n",
    "    print(\"%s CONV2D TEST %s: Time cost = %g sec/op, %g GOPS\" % (device, status, cost.mean, gops))\n",
    "\n",
    "    return correct, cost, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "021a41f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env, remote, device):\n",
    "    if device == \"vta\":\n",
    "        target = env.target\n",
    "        if env.TARGET not in [\"sim\", \"tsim\"]:\n",
    "            assert tvm.runtime.enabled(\"rpc\")\n",
    "            #program_fpga(remote, bitstream=\"/home/srchand/Desktop/research/TVM_Intel_Fork/tvm/vta/python/vta/vta.bit\")\n",
    "            #program_fpga(remote, None)\n",
    "            #program_fpga(remote, bitstream=\"/home/srchand/Desktop/research/TVM_Intel_Fork/tvm/vta/sri_scripts/bitstreams/vta_zcu104_wrapper.bit\")\n",
    "            #vta.reconfig_runtime(remote)\n",
    "    elif device == \"arm_cpu\":\n",
    "        target = env.target_vta_cpu\n",
    "    with autotvm.tophub.context(target): # load pre-tuned schedule parameters\n",
    "        for _, wl in resnet_wkls:\n",
    "            print(wl)\n",
    "            run_conv2d(env, remote, wl, target, print_ir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bd2581c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2DWorkload(batch=1, height=56, width=56, in_filter=64, out_filter=64, hkernel=3, wkernel=3, hpad=1, wpad=1, hstride=1, wstride=1)\n",
      "primfn(data_1: handle, kernel_1: handle, bias_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {compute: Buffer(compute_2: Pointer(int8), int8, [1, 4, 56, 56, 1, 16], []),\n",
      "             bias: Buffer(bias_2: Pointer(int32), int32, [1, 4, 1, 1, 1, 16], []),\n",
      "             kernel: Buffer(kernel_2: Pointer(int8), int8, [4, 4, 3, 3, 16, 16], []),\n",
      "             data: Buffer(data_2: Pointer(int8), int8, [1, 4, 56, 56, 1, 16], [])}\n",
      "  buffer_map = {data_1: data, kernel_1: kernel, bias_1: bias, compute_1: compute} {\n",
      "  attr [res: Pointer(int32)] \"storage_scope\" = \"local.acc_buffer\";\n",
      "  attr [pad_data: Pointer(int8)] \"storage_scope\" = \"local.inp_buffer\";\n",
      "  attr [kernel.local.wgt_buffer: Pointer(int8)] \"storage_scope\" = \"local.wgt_buffer\" {\n",
      "    @tir.vta.coproc_dep_push(3, 2, dtype=int32)\n",
      "    @tir.vta.coproc_dep_push(3, 2, dtype=int32)\n",
      "    for (i2.outer: int32, 0, 7) {\n",
      "      attr [IterVar(vta: int32, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 2 {\n",
      "        @tir.vta.coproc_dep_pop(3, 2, dtype=int32)\n",
      "        attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushGEMMOp\" {\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 56, 1, 0, 0, dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 2, 448, 0, 0, dtype=int32)\n",
      "          for (i.init: int32, 0, 8) {\n",
      "            @tir.vta.uop_push(0, 1, (i.init*56), 0, 0, 0, 0, 0, dtype=int32)\n",
      "          }\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "        }\n",
      "        @tir.vta.coproc_dep_push(2, 1, dtype=int32)\n",
      "      }\n",
      "      attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 2 {\n",
      "        @tir.vta.coproc_dep_pop(3, 2, dtype=int32)\n",
      "        attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushGEMMOp\" {\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 56, 1, 0, 0, dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 2, 448, 0, 0, dtype=int32)\n",
      "          for (i.init_1: int32, 0, 8) {\n",
      "            @tir.vta.uop_push(0, 1, ((i.init_1*56) + 898), 0, 0, 0, 0, 0, dtype=int32)\n",
      "          }\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "        }\n",
      "        @tir.vta.coproc_dep_push(2, 1, dtype=int32)\n",
      "      }\n",
      "      for (k_o.outer: int32, 0, 4) {\n",
      "        attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 1 {\n",
      "          @tir.vta.coproc_dep_pop(2, 1, dtype=int32)\n",
      "          @tir.call_extern(\"VTALoadBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), data_2, ((((k_o.outer*3136) + (i2.outer*448)) + (max((1 - (i2.outer*8)), 0)*56)) - 56), 56, ((10 - max((1 - (i2.outer*8)), 0)) - max(((i2.outer*8) - 47), 0)), 56, 1, max((1 - (i2.outer*8)), 0), 1, max(((i2.outer*8) - 47), 0), 0, 0, 2, dtype=int32)\n",
      "          @tir.call_extern(\"VTALoadBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), kernel_2, (k_o.outer*9), 9, 2, 36, 0, 0, 0, 0, 0, 0, 1, dtype=int32)\n",
      "          @tir.vta.coproc_dep_push(1, 2, dtype=int32)\n",
      "        }\n",
      "        attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 1 {\n",
      "          @tir.vta.coproc_dep_pop(2, 1, dtype=int32)\n",
      "          @tir.call_extern(\"VTALoadBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), data_2, ((((k_o.outer*3136) + (i2.outer*448)) + (max((1 - (i2.outer*8)), 0)*56)) - 56), 56, ((10 - max((1 - (i2.outer*8)), 0)) - max(((i2.outer*8) - 47), 0)), 56, 1, max((1 - (i2.outer*8)), 0), 1, max(((i2.outer*8) - 47), 0), 0, 580, 2, dtype=int32)\n",
      "          @tir.call_extern(\"VTALoadBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), kernel_2, ((k_o.outer*9) + 72), 9, 2, 36, 0, 0, 0, 0, 0, 18, 1, dtype=int32)\n",
      "          @tir.vta.coproc_dep_push(1, 2, dtype=int32)\n",
      "        }\n",
      "        attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 2 {\n",
      "          @tir.vta.coproc_dep_pop(1, 2, dtype=int32)\n",
      "          attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushGEMMOp\" {\n",
      "            @tir.call_extern(\"VTAUopLoopBegin\", 56, 1, 1, 0, dtype=int32)\n",
      "            @tir.call_extern(\"VTAUopLoopBegin\", 3, 0, 1, 1, dtype=int32)\n",
      "            for (d_i: int32, 0, 3) {\n",
      "              for (c_o: int32, 0, 2) {\n",
      "                for (i: int32, 0, 8) {\n",
      "                  @tir.vta.uop_push(0, 0, ((c_o*448) + (i*56)), ((i*58) + (d_i*58)), ((c_o*9) + (d_i*3)), 0, 0, 0, dtype=int32)\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "            @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "          }\n",
      "          @tir.vta.coproc_dep_push(2, 1, dtype=int32)\n",
      "        }\n",
      "        attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 2 {\n",
      "          @tir.vta.coproc_dep_pop(1, 2, dtype=int32)\n",
      "          attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushGEMMOp\" {\n",
      "            @tir.call_extern(\"VTAUopLoopBegin\", 56, 1, 1, 0, dtype=int32)\n",
      "            @tir.call_extern(\"VTAUopLoopBegin\", 3, 0, 1, 1, dtype=int32)\n",
      "            for (d_i_1: int32, 0, 3) {\n",
      "              for (c_o_1: int32, 0, 2) {\n",
      "                for (i_1: int32, 0, 8) {\n",
      "                  @tir.vta.uop_push(0, 0, (((c_o_1*448) + (i_1*56)) + 898), (((i_1*58) + (d_i_1*58)) + 580), (((c_o_1*9) + (d_i_1*3)) + 18), 0, 0, 0, dtype=int32)\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "            @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "          }\n",
      "          @tir.vta.coproc_dep_push(2, 1, dtype=int32)\n",
      "        }\n",
      "      }\n",
      "      @tir.vta.coproc_dep_pop(2, 1, dtype=int32)\n",
      "      @tir.vta.coproc_dep_pop(2, 1, dtype=int32)\n",
      "      attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 2 {\n",
      "        attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushALUOp\" {\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 896, 1, 1, 0, dtype=int32)\n",
      "          @tir.vta.uop_push(1, 0, 0, 0, 0, 3, 1, 8, dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "        }\n",
      "        @tir.call_extern(\"VTALoadBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), bias_2, 0, 2, 1, 2, 0, 0, 0, 0, 0, 896, 3, dtype=int32)\n",
      "        attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushALUOp\" {\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 2, 448, 1, 0, dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 448, 1, 0, 0, dtype=int32)\n",
      "          @tir.vta.uop_push(1, 0, 0, 896, 0, 2, 0, 0, dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "        }\n",
      "        attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushALUOp\" {\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 896, 1, 1, 0, dtype=int32)\n",
      "          @tir.vta.uop_push(1, 0, 0, 0, 0, 0, 1, 127, dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "        }\n",
      "        attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushALUOp\" {\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 896, 1, 1, 0, dtype=int32)\n",
      "          @tir.vta.uop_push(1, 0, 0, 0, 0, 1, 1, 0, dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "        }\n",
      "        @tir.vta.coproc_dep_push(2, 3, dtype=int32)\n",
      "      }\n",
      "      attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 2 {\n",
      "        attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushALUOp\" {\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 896, 1, 1, 0, dtype=int32)\n",
      "          @tir.vta.uop_push(1, 0, 898, 898, 0, 3, 1, 8, dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "        }\n",
      "        @tir.call_extern(\"VTALoadBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), bias_2, 2, 2, 1, 2, 0, 0, 0, 0, 0, 1794, 3, dtype=int32)\n",
      "        attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushALUOp\" {\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 2, 448, 1, 0, dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 448, 1, 0, 0, dtype=int32)\n",
      "          @tir.vta.uop_push(1, 0, 898, 1794, 0, 2, 0, 0, dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "        }\n",
      "        attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushALUOp\" {\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 896, 1, 1, 0, dtype=int32)\n",
      "          @tir.vta.uop_push(1, 0, 898, 898, 0, 0, 1, 127, dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "        }\n",
      "        attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushALUOp\" {\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 896, 1, 1, 0, dtype=int32)\n",
      "          @tir.vta.uop_push(1, 0, 898, 898, 0, 1, 1, 0, dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "        }\n",
      "        @tir.vta.coproc_dep_push(2, 3, dtype=int32)\n",
      "      }\n",
      "      attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 3 {\n",
      "        @tir.vta.coproc_dep_pop(2, 3, dtype=int32)\n",
      "        @tir.call_extern(\"VTAStoreBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), 0, 4, compute_2, (i2.outer*448), 448, 2, 3136, dtype=int32)\n",
      "        @tir.vta.coproc_dep_push(3, 2, dtype=int32)\n",
      "      }\n",
      "      attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 3 {\n",
      "        @tir.vta.coproc_dep_pop(2, 3, dtype=int32)\n",
      "        @tir.call_extern(\"VTAStoreBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), 898, 4, compute_2, ((i2.outer*448) + 6272), 448, 2, 3136, dtype=int32)\n",
      "        @tir.vta.coproc_dep_push(3, 2, dtype=int32)\n",
      "      }\n",
      "    }\n",
      "    @tir.vta.coproc_dep_pop(3, 2, dtype=int32)\n",
      "    @tir.vta.coproc_dep_pop(3, 2, dtype=int32)\n",
      "    @tir.vta.coproc_sync(, dtype=int32)\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTA CONV2D TEST PASSED: Time cost = 0.00182989 sec/op, 126.352 GOPS\n"
     ]
    }
   ],
   "source": [
    "run(env, remote, device=\"vta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf7e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tvm-build-il-2] *",
   "language": "python",
   "name": "conda-env-tvm-build-il-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
